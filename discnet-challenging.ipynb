{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA-DISCnet Statistics: Exercises (Challenging version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import corner\n",
    "import emcee\n",
    "import math\n",
    "import mpmath\n",
    "import numpy as np\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.special\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Maximum likelihood and Bayes example: exponential distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Suppose a set of events has a distribution of times taken for the events, and you are trying to determine the mean time for this distribution. The \u001c",
    "distribution has $p(t) \\propto e^{-t/\\tau}$. You measure $N$ event times $t_i$. If there is no limit on times that can be measured, the PDF for measured separation time $t$ is $$p(t) = \\frac{e^{-t/\\tau}}{\\tau}.$$ Show that this correctly normalises $p(t)$.\n",
    "\n",
    "Use the maximum likelihood method to show that estimates of \u001c",
    "$\\tau$ and its variance are\n",
    "given by  $$\\widehat\\tau = \\frac{1}{N} \\sum_i t_i$$ and $$V(\\tau) = \\frac{\\widehat{\\tau}^2}{N}.$$\n",
    "\n",
    "\n",
    "(b) Now suppose that you cannot measure any times longer than $T$. The truncated\n",
    "PDF (normalised to 1) is then $$p(t) =\\frac{e^{-t/\\tau}}{\\int_0^T e^{-t/\\tau} dt} =\\frac{1}{\\tau}e^{-t/\\tau}(1-e^{-T/\\tau})^{-1}.$$ By differentiating the log likelihood $l$ with respect to  $\\tau$\u001c",
    " and setting it to zero, show that the maximum-likelihood estimate of \u001c",
    "$\\tau$ is given by $$\\widehat{\\tau }=\\frac{1}{N}\\sum t_{i}+ \\frac{Te^{-T/\\widehat{\\tau }}}{\\left( 1-e^{-T/\\widehat{\\tau }}\\right) }. \\,\\,\\,(*)$$\n",
    "\n",
    "\n",
    "\n",
    "(c) Write a computer script to generate $N$ = 1000 times from the original PDF, \n",
    "with \u001c",
    " $\\tau$= 10 s with a maximum of $T$ = 15 s. Plot a histogram of the times you have\n",
    "generated.\n",
    "\n",
    "(d) Use equation (*) to estimate $\\widehat\\tau$\u001c",
    " from your generated data. Is this consistent with the true\n",
    "value, given the simplified estimate of variance on your estimate?\n",
    "\n",
    "\n",
    "\n",
    "(e) Suppose that a previous experiment has estimated the mean time to be $\\widehat\\tau=\\tau_p\\pm\\sigma$\u001b. \n",
    "Write down an expression for the posterior likelihood after measuring $N$\n",
    "times $t_i < T$, including the prior information, and maximise it with respect to \u001c",
    "$\\tau$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What maximum likelihood values do you obtain for $\\widehat\\tau$\u001c",
    " for prior estimate \u001c",
    "$\\tau_p$ = 8 s, \u001b$\\sigma$ = 1 s,\n",
    "and with computer generated data with \u001c",
    "$\\tau$ = 10 s, $T$ = 15 s for different sample sizes $N$ =\n",
    "10, 100, 1000 and 10000? Comment on your results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Bayes’ theorem example: constrained measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small quantity of powder is being weighed on a balance which gives a reading of $x\\pm\\sigma$ g.\n",
    "Assuming a uniform prior on the true mass $X$ for any positive value, $p(X > 0)$ = const, zero\n",
    "otherwise, show that the posterior likelihood for the true mass is given by\n",
    "$$p(X|x) = \\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma}\\frac{e^{-(x-X)^2/2\\sigma^2}}{\\mathrm{erfc}(-x/\\sqrt{2}\\sigma)} $$\n",
    "for X > 0, zero otherwise.\n",
    "\n",
    "Plot $p(X|x)$ for $X$ between 0 and 1 g for \u001b$\\sigma$ = 0.2 g and for mass readings $x$ = -0.3, -0.1, 0.1, 0.3 g\n",
    "(plot a separate line for each of the four readings).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 MCMC example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this question, you will write a Monte Carlo program to simulate a sample drawn from a particular parameterised PDF. In the second part, you will use the simulated data that you have generated, and do a maximum likelihood analysis to determine the parameters of the model. If\n",
    "you are successful, your analysis should yield the initial input parameters - always a useful\n",
    "check to perform when you are developing Monte Carlo routines!\n",
    "\n",
    "(a) Event Generation.\n",
    "\n",
    "i) The probability distribution function for detecting a mass $m$ in a particle physics experiment is \n",
    "\n",
    "$$p(m) = A \\frac{\\Gamma^2/4}{(m-m_0)^2+\\Gamma^2/4}$$\n",
    "\n",
    "By changing variables to $x = 2 (m - m_0)/\\Gamma$, integrate over $m$ to normalise the distribution, and thus prove that\n",
    "\u001b$$A=\\frac{2}{\\pi \\Gamma}.$$\n",
    "You will need to assume that $m_0\\gg\\Gamma$ 􀀀 in order to put sensible limits on your integral.\n",
    "\n",
    "\n",
    "\n",
    "ii) Now, given the random number generator U (0, 1) provided with any computer language, we want to generate a sample from this distribution. The best way is the transformation method; show that the transformation\n",
    "$$m = m_0 + \\frac{\\Gamma}{2}\\tan[(x-1/2)\\pi]$$\n",
    "􀀀\n",
    "generates the distribution as required. [x is a random number generated from U (0, 1) ].\n",
    "\n",
    "\n",
    "iii) Generate 10,000 masses $m$ in this way, with $m_0$ = 784 MeV, 􀀀 $\\Gamma$ = 12 MeV, and put\n",
    "them into a histogram with, say, 50 bins covering the mass range $m$ =760-810 MeV.\n",
    "\n",
    "Plot this histogram.\n",
    "\n",
    "\n",
    "(b) Now, imagine that we have carried out an experiment with a sample following this distribution. We shall use the Maximum Likelihood Method to find the best\n",
    "estimate of $m_0$ and $\\Gamma$. Note that the MLM does\n",
    "not require you to bin the data in histograms - you can work with each \"raw\" event.\n",
    "\n",
    "i) Using the normalised distribution, give\n",
    "an expression for the log likelihood for $N$ events. Write down the equations that must\n",
    "be satisfied for maximum likelihood. These are a nasty pair of implicit equations\n",
    "which you don't need to solve!\n",
    "\n",
    "\n",
    "ii) Plot log likelihood vs. mass $m_0$ in the mass range 760-810 MeV, keeping $\\Gamma$􀀀 constant at 12 MeV.\n",
    "\n",
    "See the code in breit_wigner() below.\n",
    "\n",
    "\n",
    "\n",
    "iii) Plot log likelihood vs. $\\Gamma$ 􀀀 in the range 8-16 MeV, keeping $m_0$ constant at 784\n",
    "MeV. \n",
    "\n",
    "\n",
    "iv) The variables $m_0$ and $\\Gamma$􀀀 are correlated, and to find the global maximum you generally\n",
    "need to iterate, taking it in turns to adjust each of the two variables and find the\n",
    "peak in the other. (Remember, for the plots you drew above, you already \"knew\" the\n",
    "answer for the complementary variable in each case, which is why they peaked in the\n",
    "right place straight away).\n",
    "\n",
    "Use Markov Chain Monte Carlo to explore the two-dimensional parameter space.\n",
    "Make plots of (i) the joint distribution in $m_0$ and $\\Gamma$􀀀 and (ii) marginalised distributions in $m_0$ and $\\Gamma$􀀀 separately.\n",
    "\n",
    "Use emcee to explore the parameter space.\n",
    "See http://dfm.io/emcee/current/ for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Generating arbitrarily distributed random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilising the uniform random number generator from your favourite programming language,\n",
    "write functions that will efficiently return $N$ random numbers corresponding to (a) a supplied\n",
    "empirical distribution ${x_i, p_i(x_i)}$, (b) a user-supplied 1-d PDF $p(x)$ and (c) a user-supplied\n",
    "2-d PDF $p(x, y)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
