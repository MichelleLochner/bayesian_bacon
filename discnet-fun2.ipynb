{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA-DISCnet Statistics: Exercises (Fun version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import corner\n",
    "import emcee\n",
    "import math\n",
    "import mpmath\n",
    "import numpy as np\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.special\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Maximum likelihood and Bayes example: exponential distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Suppose a set of events has a distribution of times taken for the events, and you are trying to determine the mean time for this distribution. The \u001c",
    "distribution has $p(t) \\propto e^{-t/\\tau}$. You measure $N$ event times $t_i$. If there is no limit on times that can be measured, the PDF for measured separation time $t$ is $$p(t) = \\frac{e^{-t/\\tau}}{\\tau}.$$ If you like, you can integrate this to show that it is normalised to 1.\n",
    "\n",
    "Using the maximum likelihood method it is possible to show that estimates of \u001c",
    "$\\tau$ and its variance are\n",
    "given by  $$\\widehat\\tau = \\frac{1}{N} \\sum_i t_i$$ and $$V(\\tau) = \\frac{\\widehat{\\tau}^2}{N}.$$\n",
    "\n",
    "\n",
    "Now suppose that you cannot measure any times longer than $T$. The truncated\n",
    "PDF (normalised to 1) is then $$p(t) =\\frac{e^{-t/\\tau}}{\\int_0^T e^{-t/\\tau} dt} =\\frac{1}{\\tau}e^{-t/\\tau}(1-e^{-T/\\tau})^{-1}.$$ By differentiating the log likelihood $l$ with respect to  $\\tau$\u001c",
    " and setting it to zero, one can show that the maximum-likelihood estimate of \u001c",
    "$\\tau$ is given by $$\\widehat{\\tau }=\\frac{1}{N}\\sum t_{i}+ \\frac{Te^{-T/\\widehat{\\tau }}}{\\left( 1-e^{-T/\\widehat{\\tau }}\\right) }. \\,\\,\\,(*)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(b) Below you'll see exp_trunc(), which generates $N$ = 1000 times from the original PDF, \n",
    "with \u001c",
    " $\\tau$= 10 s with a maximum of $T$ = 15 s. It plots a histogram of generated times. Work together to understand the code, and try different values of the parameters. You could also explore implementing different PDFs, using the transformation method. For instance, one can generate exponentially distributed\n",
    "  random numbers $t$ from a uniform distribution $x$:\n",
    "\\begin{align*}\n",
    "  x &= \\int_0^t dt \\frac{1}{\\tau} e^{-t/\\tau} = \\left[-e^{-t/\\tau}\\right]_0^t\n",
    "      = 1 - e^{-t/\\tau}\\\\\n",
    "  t &= -\\tau \\ln(1-x).\n",
    "\\end{align*}\n",
    "You could dream up alternative distributions and see what happens. More on this in question 4 below.\n",
    "\n",
    "\n",
    "(c) exp_trunc also implements equation (*) to estimate $\\widehat\\tau$\u001c",
    " from the generated data. Try to understand how the code is doing this (and experiment with changing the code for this estimator). Is this consistent with the true\n",
    "value of $\\tau$, given the simplified estimate of variance on the estimate?\n",
    "\n",
    "\n",
    "(d) Suppose that a previous experiment has estimated the mean time to be $\\widehat\\tau=\\tau_p\\pm\\sigma$\u001b. This can act as a prior. It is possible to write down the posterior  by multiplying the likelihood from the new observations by the prior. Then one can differentiate the log likelihood, set it to zero, and hence find a maximum likelihood estimate for $\\tau$.\n",
    "This is found to be \n",
    "\\begin{equation}\n",
    "\\widehat{\\tau }=\\frac{1}{N}\\sum t_{i}+ \\frac{Te^{-T/\\widehat{%\n",
    "      \\tau }}}{\\left( 1-e^{-T/\\widehat{\\tau }}\\right) } -\n",
    "\\frac{1}{N} \\frac{(\\tau - \\tau_P)\\tau^2}{\\sigma^2}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "What maximum likelihood values do you obtain for $\\widehat\\tau$\u001c",
    " for prior estimate \u001c",
    "$\\tau_p$ = 8 s, \u001b$\\sigma$ = 1 s,\n",
    "and with computer generated data with \u001c",
    "$\\tau$ = 10 s, $T$ = 15 s for different sample sizes $N$ =\n",
    "10; 100; 1000 and 10000? Comment on your results. Try different values of the parameters, and try altering the code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp_trunc(nran=1000, tau=10, T=15, tau_p=8, sigma=1):\n",
    "    \"\"\"Truncated exponential distribution.\"\"\"\n",
    "    t = np.random.exponential(tau, nran)\n",
    "    t = t[t < T]\n",
    "    tt = -tau*np.log(1 - np.random.random(nran))\n",
    "    tt = tt[tt < T]\n",
    "    nuse = len(t)\n",
    "    plt.clf()\n",
    "    plt.hist(t, 15)\n",
    "#    plt.hist(tt, 15)\n",
    "    plt.xlabel('t [s]')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    tmean = t.mean()\n",
    "\n",
    "    def fn(te):\n",
    "        return tmean + T*math.exp(-T/te) / (1 - math.exp(-T/te)) - te\n",
    "\n",
    "    def fn_prior(te):\n",
    "        return (tmean + T*math.exp(-T/te) / (1 - math.exp(-T/te))\n",
    "                - (te - tau_p)*te**2 / (nuse*sigma**2) - te)\n",
    "\n",
    "    res = scipy.optimize.fsolve(fn, tmean)\n",
    "    print('tmean = {:4.1f}, maxL solution = {:4.1f}, est std = {:4.1f}'.format(\n",
    "            tmean, res[0], res[0]/math.sqrt(nuse)))\n",
    "    res = scipy.optimize.fsolve(fn_prior, tmean)\n",
    "    print('With prior maxL solution = {:4.1f}, est std = {:4.1f}'.format(\n",
    "            res[0], res[0]/math.sqrt(nuse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmean =  5.5, maxL solution =  9.2, est std =  0.3\n",
      "With prior maxL solution =  8.8, est std =  0.3\n"
     ]
    }
   ],
   "source": [
    "exp_trunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmean =  5.8, maxL solution = 10.4, est std =  1.2\n",
      "With prior maxL solution =  8.4, est std =  0.9\n"
     ]
    }
   ],
   "source": [
    "# Try calling exp_trunc with different values of N.  \n",
    "# For low N, you should find that the estimate is pulled down by the prior, \n",
    "# but as N increases, the estimate should tend to the 'true' value.\n",
    "exp_trunc(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Bayes’ theorem example: constrained measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small quantity of powder is being weighed on a balance which gives a reading of $x\\pm\\sigma$ g.\n",
    "Assuming a uniform prior on the true mass $X$ for any positive value, $p(X > 0)$ = const, zero\n",
    "otherwise, we can show that the posterior likelihood for the true mass is given by\n",
    "$$p(X|x) = \\sqrt{\\frac{2}{\\pi}}\\frac{1}{\\sigma}\\frac{e^{-(x-X)^2/2\\sigma^2}}{\\mathrm{erfc}(-x/\\sqrt{2}\\sigma)} $$\n",
    "for X > 0, zero otherwise. The argument is given below - try to follow it. (Don't worry if you feel that you'd never have thought of this - we gradually learn these tricks over years and years!)\n",
    "\n",
    "\n",
    "For measurement $x$ and theory $X$, Bayes' theorem states\n",
    "  \\begin{equation}\n",
    "    p(X|x) = \\frac{p(x|X) p(X)}{p(x)}.\n",
    "  \\end{equation}\n",
    "\n",
    "  For Gaussian measurement errors and a constant prior for positive $X$,\n",
    "  we have:\n",
    "  \\begin{align*}\n",
    "    p(x|X) &= \\frac{e^{-(x-X)^2/2\\sigma^2}}{\\sigma \\sqrt{2 \\pi}},\\\\\n",
    "    p(X) &= \\mbox{ const if } X > 0, \\mbox{ zero otherwise},\\\\\n",
    "    p(x) &= \\int_{-\\infty}^\\infty p(x|X) p(X) dX = \\int_0^\\infty p(x|X) dX.\n",
    "  \\end{align*}\n",
    "  We thus have\n",
    "\\begin{equation}\n",
    "p\\left( X|x\\right) =\\frac{e^{-\\left( x-X\\right) ^{2}/2\\sigma ^{2}}}\n",
    "{\\int_{0}^{\\infty }e^{-( x-X) ^{2}/2\\sigma^{2}}dX}\n",
    "\\mbox{ for $X > 0$, zero otherwise.}\n",
    "\\end{equation}\n",
    "\n",
    "Now, substituting $y = (x - X)/\\sqrt{2} \\sigma$,\n",
    "\\begin{equation}\n",
    "\\int_{0}^{\\infty }e^{-( x-X) ^{2}/2\\sigma^{2}}dX =\n",
    "\\sqrt{2} \\sigma \\int_{-x/\\sqrt{2} \\sigma}^\\infty e^{-y^2} dy =\n",
    "\\sqrt{2} \\sigma \\frac{\\sqrt{\\pi}}{2} {\\rm erfc} (-x/\\sqrt{2} \\sigma),\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "{\\rm erfc}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_x^\\infty e^{-t^2} dt.\n",
    "\\end{equation}\n",
    "\n",
    "Thus\n",
    "\\begin{equation}\n",
    "p(X|x) =\\sqrt{\\frac{2}{\\pi}} \\frac{1}{\\sigma} \\frac{e^{-\\left( x-X\\right) ^{2}/2\\sigma ^{2}}}\n",
    "{{\\rm erfc} (-x/\\sqrt{2} \\sigma)}\n",
    "\\mbox{ for $X > 0$, zero otherwise.}\n",
    "\\end{equation}\n",
    "\n",
    "Bayes() below makes plots of $p(X|x)$. Try to understand how it works. Plot for $X$ between 0 and 1 g for \u001b$\\sigma$ = 0.2 g and for mass readings $x$ = -0.3, -0.1, 0.1, 0.3 g\n",
    "(plot a separate line for each of the four readings).\n",
    "\n",
    "Try different readings, and experiment with what happens for different PDFs and priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayes(sigma=0.2, X=np.linspace(0.001, 1.0, 50)):\n",
    "    \"\"\"Bayesian limits on mass (Q3)\"\"\"\n",
    "\n",
    "    def p(X, x, sigma):\n",
    "        prob = np.zeros(len(X))\n",
    "        idx = X > 0\n",
    "        prob[idx] = (np.sqrt(2.0/math.pi) / sigma * np.exp(-(x - X[idx])**2/(2 * sigma**2))/\n",
    "                     scipy.special.erfc(-x/(np.sqrt(2.0) * sigma)))\n",
    "        return prob\n",
    "\n",
    "    plt.clf()\n",
    "    for x in (-0.3, -0.1, 0.1, 0.3):\n",
    "        plt.plot(X, p(X, x, sigma), label='x = {}'.format(x))\n",
    "    plt.xlabel(r'$X$')\n",
    "    plt.ylabel(r'$P(X|x)$')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 MCMC example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this question, we use a Monte Carlo program to simulate a sample drawn from a particular parameterised PDF. In the second part, we use the simulated data that we have generated, and do a maximum likelihood analysis to determine the parameters of the model. If\n",
    "we are successful, our analysis should yield the initial input parameters - always a useful\n",
    "check to perform when we are developing Monte Carlo routines!\n",
    "\n",
    "(a) Event Generation.\n",
    "\n",
    "i) The probability distribution function for detecting a mass $m$ in a particle physics experiment is \n",
    "\n",
    "$$p(m) = A \\frac{\\Gamma^2/4}{(m-m_0)^2+\\Gamma^2/4}$$\n",
    "\n",
    "where $m_0$ and $\\Gamma$ are parameters of the PDF. By changing variables to $x = 2 (m - m_0)/\\Gamma$, and integrating over $m$ to normalise the distribution, we can find that\n",
    "\u001b$$A=\\frac{2}{\\pi \\Gamma}.$$ (You might like to do this).\n",
    "\n",
    "\n",
    "ii) Now, given a random number generator U (0, 1), we want to generate a sample from this distribution. The best way is the transformation method; it is possible to show that the transformation\n",
    "$$m = m_0 + \\frac{\\Gamma}{2}\\tan[(x-1/2)\\pi]$$\n",
    "􀀀\n",
    "generates the distribution as required. [x is a random number generated from U (0, 1) ].\n",
    "\n",
    "See the code in breit_wigner() below which does this. Understand how it's implemented.\n",
    "\n",
    "\n",
    "iii) The code now generates 10,000 masses $m$ in this way, with $m_0$ = 784 MeV, 􀀀 $\\Gamma$ = 12 MeV, and puts\n",
    "them into a histogram with, say, 50 bins covering the mass range $m$ =760-810 MeV. Experiment with this part of the code, changing the histogram bin size, sample size or parameters $m_0$ and $\\Gamma$.\n",
    "\n",
    "\n",
    "(b) Now, imagine that we have carried out an experiment with a sample following this distribution. We shall use the Maximum Likelihood Method to find the best\n",
    "estimate of $m_0$ and $\\Gamma$). \n",
    "\n",
    "i) Since the normalised distribution is \n",
    "\\begin{equation}\n",
    "p(m) =\\frac{2}{\\pi \\Gamma }\\frac{\\Gamma ^{2}/4}{\\left( m-m_{0}\\right)\n",
    "^{2}+\\Gamma ^{2}/4},\n",
    "\\end{equation}\n",
    "make sure that you can see that the log likelihood is \n",
    "\\begin{equation}\n",
    "l=-N\\ln 2\\pi +\\sum_{i=1}^{N}\\ln \\left\\{ \\frac{\\Gamma }{\\left(\n",
    "m_{i}-m_{0}\\right) ^{2}+\\Gamma ^{2}/4}\\right\\} .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "ii) Use the code below to plot log likelihood vs. mass $m_0$ in the mass range 760-810 MeV, keeping $\\Gamma$􀀀 constant at 12 MeV. Understand how the code is doing this, and try changing the range and $\\Gamma$.\n",
    "\n",
    "\n",
    "iii) Similarly use the code to plot log likelihood vs. $\\Gamma$ 􀀀 in the range 8-16 MeV, keeping $m_0$ constant at 784\n",
    "MeV. Try changing range and $m_0$.\n",
    "\n",
    "\n",
    "iv) The variables $m_0$ and $\\Gamma$􀀀 are correlated, and to find the global maximum you generally\n",
    "need to iterate, taking it in turns to adjust each of the two variables and find the\n",
    "peak in the other. (Remember, for the plots you drew above, you already \"knew\" the\n",
    "answer for the complementary variable in each case, which is why they peaked in the\n",
    "right place straight away).\n",
    "\n",
    "The code uses Markov Chain Monte Carlo to explore the two-dimensional parameter space.\n",
    "It can make plots of (i) the joint distribution in $m_0$ and $\\Gamma$􀀀 and (ii) marginalised distributions in $m_0$ and $\\Gamma$􀀀 separately.\n",
    "\n",
    "See http://dfm.io/emcee/current/ for documentation.\n",
    "\n",
    "Try changing the code in various ways to understand how it's working - e.g. change the number of walkers, the length of chains, the starting points. See if you can implement MCMC for a different log likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def breit_wigner(N=10000, m_true=784, gam_true=12, m_range=(783, 785),\n",
    "                 gam_range=(11, 13), nchain=3, nstep=1000):\n",
    "    \"\"\"Simulation of Breit-Wigner distribution.\"\"\"\n",
    "\n",
    "    ln2pi = math.log(2*math.pi)\n",
    "\n",
    "    def loglike(theta):\n",
    "        m0 = theta[0]\n",
    "        gamma = theta[1]\n",
    "        mgrid, m0grid, gamgrid = np.meshgrid(m, m0, gamma)\n",
    "        return np.squeeze(\n",
    "            N*ln2pi +\n",
    "            np.sum(np.log(gamgrid / ((mgrid-m0grid)**2 + gamgrid**2/4.0)),\n",
    "                   axis=1))\n",
    "\n",
    "    def lnprob(theta):\n",
    "        m0 = math.exp(theta[0])\n",
    "        gamma = math.exp(theta[1])\n",
    "        ll = -N*ln2pi + np.sum(np.log(gamma / ((m-m0)**2 + gamma**2/4.0)))\n",
    "#        print(theta, ll)\n",
    "        return ll\n",
    "\n",
    "    x = np.random.rand(N)\n",
    "    m = m_true + 0.5*gam_true*np.tan((x-0.5)*math.pi)\n",
    "    plt.clf()\n",
    "    plt.hist(m, 50, (760, 810))\n",
    "    plt.xlabel(r'$m$ (MeV)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    mbins = np.linspace(m_range[0], m_range[1], 50)\n",
    "    ll = loglike((mbins, gam_true))\n",
    "    plt.clf()\n",
    "    plt.plot(mbins, ll)\n",
    "    plt.xlabel(r'$m_0$ (MeV)')\n",
    "    plt.ylabel('log likelihood')\n",
    "    plt.show()\n",
    "\n",
    "    gambins = np.linspace(gam_range[0], gam_range[1], 50)\n",
    "    ll = loglike((m_true, gambins))\n",
    "    plt.clf()\n",
    "    plt.plot(gambins, ll)\n",
    "    plt.xlabel(r'$\\Gamma$ (MeV)')\n",
    "    plt.ylabel('log likelihood')\n",
    "    plt.show()\n",
    "\n",
    "    ndim = 2\n",
    "    x0 = (7, 3)\n",
    "    nwalkers = 10*ndim\n",
    "    pos = np.tile(x0, (nwalkers, 1)) + 0.1*np.random.randn(nwalkers, ndim)\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob)\n",
    "    for ichain in range(nchain):\n",
    "        sampler.reset()\n",
    "        pos, prob, state = sampler.run_mcmc(pos, nstep)\n",
    "        print(\"Mean acceptance fraction: {0:.3f}\"\n",
    "              .format(np.mean(sampler.acceptance_fraction)))\n",
    "        try:\n",
    "            print(\"Autocorrelation time:\", sampler.get_autocorr_time(c=1))\n",
    "        except:\n",
    "            print(\"Unable to compute autocorrelation time\")\n",
    "        plt.clf()\n",
    "        nrow = 2\n",
    "        ncol = 1\n",
    "        fig, axes = plt.subplots(nrow, ncol, num=1)\n",
    "        fig.subplots_adjust(hspace=0, wspace=0.1)\n",
    "        fig.text(0.5, 0.02, 'Step number', ha='center', va='center')\n",
    "        fig.text(0.06, 0.7, r'$\\ln m_0$', ha='center',\n",
    "                 va='center', rotation='vertical')\n",
    "\n",
    "        fig.text(0.06, 0.3, r'$\\ln \\Gamma$', ha='center',\n",
    "                 va='center', rotation='vertical')\n",
    "        for ipar in range(ndim):\n",
    "            ax = axes[ipar]\n",
    "            for iw in range(nwalkers):\n",
    "                ax.plot(sampler.chain[iw, :, ipar])\n",
    "#                if (iw == 0):\n",
    "#                    print(sampler.chain[iw, :, ipar])\n",
    "        plt.show()\n",
    "    res = np.exp(np.array(np.percentile(sampler.flatchain, [50, 16, 84], axis=0)))\n",
    "    print(res)\n",
    "\n",
    "    samples = np.exp(sampler.chain.reshape((-1, ndim)))\n",
    "    fig = corner.corner(samples, labels=[\"$m_0$\", \"$\\Gamma$\"],\n",
    "                        truths=[m_true, gam_true])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean acceptance fraction: 0.696\n",
      "Autocorrelation time: [ 20.85805907  50.95038131]\n",
      "Mean acceptance fraction: 0.724\n",
      "Autocorrelation time: [ 21.21932431  29.78205562]\n",
      "Mean acceptance fraction: 0.710\n",
      "Autocorrelation time: [ 49.79526999  40.95463164]\n",
      "[[ 783.94104018   11.88444898]\n",
      " [ 783.85331907   11.71240283]\n",
      " [ 784.02275372   12.06043692]]\n",
      "[[  96.96090274   96.96090274   97.04349668 ...,   97.15531145\n",
      "    97.15531145   97.15531145]\n",
      " [  23.40958227   23.40958227   23.44316534 ...,   23.49166816\n",
      "    23.49166816   23.49166816]\n",
      " [ 401.6054858   401.6054858   401.71368118 ...,  401.80861061\n",
      "   401.80861061  401.80861061]]\n"
     ]
    }
   ],
   "source": [
    "breit_wigner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Generating arbitrarily distributed random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilising the uniform random number generator from Python, we can\n",
    "write functions that will efficiently return $N$ random numbers corresponding to (a) a supplied\n",
    "empirical distribution ${x_i, p_i(x_i)}$, (b) a user-supplied 1-d PDF $p(x)$ and (c) a user-supplied\n",
    "2-d PDF $p(x, y)$.\n",
    "\n",
    "This is implemented in ran_fun_test() below. Work together to understand the code, and experiment with different PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ran_dist(x, p, nran):\n",
    "    \"\"\"Generate nran random points according to distribution p(x)\"\"\"\n",
    "\n",
    "    if np.amin(p) < 0:\n",
    "        print('ran_dist warning: pdf contains negative values!')\n",
    "    cp = np.cumsum(p)\n",
    "    y = (cp - cp[0]) / (cp[-1] - cp[0])\n",
    "    r = np.random.random(nran)\n",
    "    return np.interp(r, y, x)\n",
    "\n",
    "\n",
    "def ran_fun(f, xmin, xmax, nran, args=None, nbin=1000):\n",
    "    \"\"\"Generate nran random points according to pdf f(x)\"\"\"\n",
    "\n",
    "    x = np.linspace(xmin, xmax, nbin)\n",
    "    if args:\n",
    "        p = f(x, *args)\n",
    "    else:\n",
    "        p = f(x)\n",
    "    return ran_dist(x, p, nran)\n",
    "\n",
    "\n",
    "def ran_fun2(f, xmin, xmax, ymin, ymax, nran, args=(), nbin=1000, pplot=False):\n",
    "    \"\"\"Generate nran random points according to 2d pdf f(x,y)\"\"\"\n",
    "\n",
    "    dx = float(xmax - xmin)/nbin\n",
    "    dy = float(ymax - ymin)/nbin\n",
    "    x = np.linspace(xmin + 0.5*dx, xmax - 0.5*dx, nbin)\n",
    "    y = np.linspace(ymin + 0.5*dy, ymax - 0.5*dy, nbin)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    p = f(xv, yv, *args)\n",
    "    if pplot:\n",
    "        plt.clf()\n",
    "        plt.imshow(p, aspect='auto', origin='lower',\n",
    "                   extent=(xmin, xmax, ymin, ymax), interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "    binno = range(nbin * nbin)\n",
    "    bins = (ran_dist(binno, p, nran)).astype(int)\n",
    "    j = bins // nbin\n",
    "    i = bins % nbin\n",
    "    xran = x[i]\n",
    "    yran = y[j]\n",
    "\n",
    "    # Add uniform random offsets to avoid quantization\n",
    "    xoff = dx * (np.random.random(nran) - 0.5)\n",
    "    yoff = dy * (np.random.random(nran) - 0.5)\n",
    "\n",
    "    return xran + xoff, yran + yoff\n",
    "\n",
    "\n",
    "def ran_fun_test():\n",
    "    \"\"\"Test ran_fun2\"\"\"\n",
    "\n",
    "    def fun(x, y):\n",
    "        return np.cos(x)**2 * np.sin(y)**2\n",
    "\n",
    "    xr, yr = ran_fun2(fun, -5, 5, -5, 5, 10000, pplot=True)\n",
    "    plt.scatter(xr, yr, 0.1, c='w')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ran_fun_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
